var documenterSearchIndex = {"docs":
[{"location":"getting_started/#Getting-started","page":"Getting Started","title":"Getting started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This package provides multiple algorithms for Blind Source Separation.  At this time the user can choose between JADE, Shibbs and Picard. ","category":"page"},{"location":"getting_started/#How-to-use-the-datatype-SensorData","page":"Getting Started","title":"How to use the datatype SensorData","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Key to using the package is the SensorData datatype. The user can either create a SensorData instance by loading it from disk using the read_dataset() function, or they can construct their own by provididing an array of timestamps of length N and a corresponding matrix of size Ntimes M, where M is the amount of sensors in the dataset. When the user has created a SensorData instance, they might plot it or perform Blind Source separation using the perform_separation function.","category":"page"},{"location":"getting_started/#Code-Example","page":"Getting Started","title":"Code Example","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Load a dataset from disk","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"x = read_dataset(\"data/foetal_ecg.dat\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Plot dataset","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"plot_dataset(x; title=\"ECG Sensor Data\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Prepare JADE algorithm","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"algo = Jade(2)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Prepare Shibbs algorithm","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"algo = Shibbs(2, 1000, 1)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Prepare Picard algorithm","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"algo = Picard(2, 3, 200, 1e-6, 1e-2, 10, false)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Run source separation. This yields the separated dataset as well as the unmixing matrix","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"y, S = perform_separation(x, algo)","category":"page"},{"location":"getting_started/#Complete-example:","page":"Getting Started","title":"Complete example:","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This example plots the original data, the whitened data, as well as the results of Jade, Shibbs and Picard algorithm.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"x = read_dataset(\"data/foetal_ecg.dat\")\nplot_dataset(x; title=\"ECG Sensor Data\")\n\nx_white = whiten_dataset(x)\nplot_dataset(x_white; title=\"Whitened Sensor Data\")\n\nalgo = Jade(2)\na, _ = perform_separation(x, algo)\nplot_dataset(a; title=\"Jade - Estimated Source Signals\")\n\nalgo = Shibbs(2, 1000, 1)\nb, _ = perform_separation(x, algo)\nplot_dataset(b; title=\"Shibbs - Estimated Source Signals\")\n\nalgo = Picard(2, 3, 200, 1e-6, 1e-2, 10, true)\nc, _ = perform_separation(x, algo)\nplot_dataset(c; title=\"Picard - Estimated Source Signals\")","category":"page"},{"location":"shibbs/#Shibbs","page":"Shibbs","title":"Shibbs","text":"","category":"section"},{"location":"shibbs/#Description","page":"Shibbs","title":"Description","text":"","category":"section"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"The Shibbs algorithm is an iterative method used for blind source separation through joint approximate diagonalization of a set of covariance matrices. \nIt aims to find a linear transformation that simultaneously diagonalizes these matrices, thereby recovering statistically independent source signals from observed mixtures. By applying successive Givens rotations, Shibbs progressively reduces off-diagonal elements, enhancing signal separation quality. This approach is particularly effective when sources exhibit distinct covariance structures, and its iterative nature allows refinement over multiple passes to improve accuracy. Shibbs is widely utilized in signal processing and data analysis tasks where separating mixed signals without prior information is essential.","category":"page"},{"location":"shibbs/#Pros","page":"Shibbs","title":"Pros","text":"","category":"section"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"Effective for Blind Source Separation: Can separate mixed signals without prior knowledge about the sources.","category":"page"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"Joint Diagonalization: Works well when multiple covariance matrices represent the data, improving separation quality.","category":"page"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"Iterative Refinement: Can be run multiple times to improve accuracy.","category":"page"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"Relatively Simple Updates: Uses Givens rotations, which are computationally efficient and stable.","category":"page"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"Adaptable: Thresholds and iteration limits allow tuning based on data and desired precision.","category":"page"},{"location":"shibbs/#Cons","page":"Shibbs","title":"Cons","text":"","category":"section"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"Sensitivity to Initialization: The starting point can affect the final output, sometimes leading to suboptimal local minima.","category":"page"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"Computational Cost: For large datasets or high-dimensional data, iterative updates can be time-consuming.","category":"page"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"Parameter Sensitivity: Choice of thresholds and max iterations needs careful tuning, as poor choices can slow convergence or degrade results.","category":"page"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"No Guarantee of Global Optimum: Like many iterative methods, it may converge to local optima rather than the best possible solution.","category":"page"},{"location":"shibbs/","page":"Shibbs","title":"Shibbs","text":"Convergence Issues: May require multiple runs or iterations to reach a good solution, especially with noisy or ill-conditioned data. As can be seen in the following diagram, only iteration 24 provided the correct separation. (Image: Demonstration of convergence issue)","category":"page"},{"location":"jade/#JADE","page":"JADE","title":"JADE","text":"","category":"section"},{"location":"jade/#Description","page":"JADE","title":"Description","text":"","category":"section"},{"location":"jade/","page":"JADE","title":"JADE","text":"The JADE (Joint Approximate Diagonalization of Eigenmatrices) algorithm is a popular method for blind source separation, particularly in the context of Independent Component Analysis (ICA). \nIt works by exploiting higher-order statistical moments, specifically, fourth-order cumulants, to identify a linear transformation that makes the separated signals as statistically independent as possible. JADE begins with a whitening step and proceeds by jointly diagonalizing a set of cumulant matrices. This approach is especially effective when non-Gaussianity plays a crucial role in distinguishing sources. Due to its reliance on cumulants, JADE can detect subtle statistical structures that second-order methods might miss, making it a powerful tool for processing real-world mixed signals.","category":"page"},{"location":"jade/#Pros","page":"JADE","title":"Pros","text":"","category":"section"},{"location":"jade/","page":"JADE","title":"JADE","text":"Powerful for ICA: Utilizes fourth-order statistics, enabling robust separation of non-Gaussian sources.","category":"page"},{"location":"jade/","page":"JADE","title":"JADE","text":"No Prior Information Needed: Operates in a fully blind setup without requiring knowledge of the source signals or mixing process.","category":"page"},{"location":"jade/","page":"JADE","title":"JADE","text":"Joint Diagonalization of Cumulants: Offers improved performance in separating statistically independent components.","category":"page"},{"location":"jade/","page":"JADE","title":"JADE","text":"Well-Established: Widely studied and validated in literature. Known for reliable and consistent results in many domains.","category":"page"},{"location":"jade/","page":"JADE","title":"JADE","text":"Effective Whitening Preprocessing: Reduces dimensionality and prepares data for optimal separation.","category":"page"},{"location":"jade/#Cons","page":"JADE","title":"Cons","text":"","category":"section"},{"location":"jade/","page":"JADE","title":"JADE","text":"Computational Complexity: Calculation and diagonalization of fourth-order cumulant matrices can be computationally intensive.","category":"page"},{"location":"jade/","page":"JADE","title":"JADE","text":"Memory Usage: Storing and processing multiple cumulant matrices can require substantial memory, especially for high-dimensional data.","category":"page"},{"location":"jade/","page":"JADE","title":"JADE","text":"Less Effective for Gaussian Sources: Assumes non-Gaussianity. Performance degrades when sources are close to Gaussian.","category":"page"},{"location":"jade/","page":"JADE","title":"JADE","text":"Fixed-Point Performance: Lacks adaptability in convergence behavior compared to some gradient-based or iterative refinement methods.","category":"page"},{"location":"jade/","page":"JADE","title":"JADE","text":"Scalability Limitations: May struggle with very large datasets due to high cost of cumulant computation and matrix operations.","category":"page"},{"location":"test_results/#Test-Results","page":"Test Results","title":"Test Results","text":"","category":"section"},{"location":"test_results/","page":"Test Results","title":"Test Results","text":"Each algorithm is tested upon the same minimal Data: a Sinvewave and a Squarewave mixed together. (Image: Mixed Testdata) \n","category":"page"},{"location":"test_results/#JADE","page":"Test Results","title":"JADE","text":"","category":"section"},{"location":"test_results/","page":"Test Results","title":"Test Results","text":"The Jade algorithm has no issues recovering the original sources: (Image: Jade Recovery) \n","category":"page"},{"location":"test_results/#SHIBBS","page":"Test Results","title":"SHIBBS","text":"","category":"section"},{"location":"test_results/","page":"Test Results","title":"Test Results","text":"Shibbs, as an iterative algorithm, needs multiple runs to find the original sources: (Image: Shibbs Recovery) \n","category":"page"},{"location":"test_results/#PICARD","page":"Test Results","title":"PICARD","text":"","category":"section"},{"location":"test_results/","page":"Test Results","title":"Test Results","text":"The Picard algorithm, like Jade, recovers the original sources faithfully: (Image: Picard Recovery)","category":"page"},{"location":"picard/#Picard","page":"Picard","title":"Picard","text":"","category":"section"},{"location":"picard/#Description","page":"Picard","title":"Description","text":"","category":"section"},{"location":"picard/","page":"Picard","title":"Picard","text":"Picard (Preconditioned ICA for Real Data) is an advanced algorithm for Independent Component Analysis that accelerates convergence using quasi-Newton optimization techniques. \nUnlike traditional ICA methods that rely purely on fixed-point or gradient-based updates, Picard employs preconditioning and curvature information to optimize the likelihood function more efficiently. It models the data as a linear mixture of independent, non-Gaussian sources and seeks to find the unmixing matrix by maximizing the log-likelihood under certain contrast functions. Designed to be robust and fast on real-world datasets, Picard integrates ideas from L-BFGS and uses Hessian approximations to handle poorly conditioned or noisy data more effectively than classic ICA approaches.","category":"page"},{"location":"picard/#Pros","page":"Picard","title":"Pros","text":"","category":"section"},{"location":"picard/","page":"Picard","title":"Picard","text":"Fast Convergence: Quasi-Newton updates significantly accelerate convergence, especially on large or complex datasets.","category":"page"},{"location":"picard/","page":"Picard","title":"Picard","text":"Robust on Real Data: Designed to handle the imperfections and variability typical of real-world signals.","category":"page"},{"location":"picard/","page":"Picard","title":"Picard","text":"Improved Stability: Uses preconditioning and curvature information to reduce the risk of divergence or poor local minima.","category":"page"},{"location":"picard/","page":"Picard","title":"Picard","text":"Compatibility: Can be used as a drop-in replacement for FastICA or other ICA methods in many pipelines.","category":"page"},{"location":"picard/","page":"Picard","title":"Picard","text":"Flexible Contrast Functions: Supports various contrast functions tailored to different source distributions.","category":"page"},{"location":"picard/#Cons","page":"Picard","title":"Cons","text":"","category":"section"},{"location":"picard/","page":"Picard","title":"Picard","text":"More Complex Implementation: Involves sophisticated optimization techniques, making it harder to implement from scratch.","category":"page"},{"location":"picard/","page":"Picard","title":"Picard","text":"Higher Memory Use: Maintains curvature information and history (e.g., in L-BFGS), which may increase memory consumption.","category":"page"},{"location":"picard/","page":"Picard","title":"Picard","text":"Sensitive to Contrast Choice: Performance can vary depending on the selected contrast function and source characteristics.","category":"page"},{"location":"picard/","page":"Picard","title":"Picard","text":"Slower for Small Problems: May be overkill for low-dimensional or well-conditioned datasets where simpler ICA methods suffice.","category":"page"},{"location":"picard/","page":"Picard","title":"Picard","text":"Requires Whitening: Like most ICA methods, it needs a preprocessing step to whiten the data before separation.","category":"page"},{"location":"#ICA_BlindSourceSeparation","page":"Home","title":"ICA_BlindSourceSeparation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for ICA_BlindSourceSeparation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#ICA_BlindSourceSeparation.demo-Tuple{}","page":"Home","title":"ICA_BlindSourceSeparation.demo","text":"demo(; return_data = false)\n\nCompares Jade, Shibbs, Picard algorithms for separating ECG data.\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.estimate_cumulants-Tuple{AbstractMatrix}","page":"Home","title":"ICA_BlindSourceSeparation.estimate_cumulants","text":"estimate_cumulants(X::AbstractMatrix)\n\nReturns cumulant matrix.\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.gradient-Tuple{AbstractMatrix{<:Real}, AbstractMatrix{<:Real}}","page":"Home","title":"ICA_BlindSourceSeparation.gradient","text":"gradient(Y, psiY)\n\nCompute the gradient of the contrast function with respect to the input signals.\n\nArguments\n\nY::AbstractMatrix{<:Real}: an N×T matrix where each row is a signal component over T samples.\npsiY::AbstractMatrix{<:Real}: the elementwise derivative of the contrast function, same size as Y.\n\nReturns\n\nAn N×N matrix representing the relative gradient\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.ica_jade-Tuple{sensorData, Int64}","page":"Home","title":"ICA_BlindSourceSeparation.ica_jade","text":"ica_jade(dataset::sensorData, m::Int)\n\nSeparates m mixed sources with JADE algorithm. Returns separated sources and transformation matrix V.\n\nSee also whiten_dataset, estimate_cumulants, joint_diagonalize\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.ica_picard-Tuple{sensorData, Int64, Int64, Int64, Real, Real, Int64}","page":"Home","title":"ICA_BlindSourceSeparation.ica_picard","text":"ica_picard(dataset::sensorData, m::Int, maxiter::Int, tol::Real, lambda_min::Real, ls_tries::Int; verbose::Bool=false)\n\nPerform Independent Component Analysis (ICA) using the Picard algorithm with limited-memory BFGS optimization.\n\nArguments\n\ndataset::sensorData\nm::Int: Size of L-BFGS's memory. Typical values are in the range 3-15\nmaxiter::Int: Maximal number of iterations\ntol::real: tolerance for the stopping criterion.    Iterations stop when the norm of the projected gradient gets smaller than tol.\nlambda_min::Real: Minimum eigenvalue for regularizing the Hessian approximation.\nls_tries::Int: Number of tries allowed for the backtracking line-search.   When that number is exceeded, the direction is thrown away and the gradient is used instead.\nverbose::Bool=false: If true, prints the informations about the algorithm.\n\nReturns\n\nsensorData: A new sensorData object containing the unmixed data.\nW: The unmixing matrix.\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.ica_shibbs","page":"Home","title":"ICA_BlindSourceSeparation.ica_shibbs","text":"ica_shibbs(dataset::sensorData, m::Integer, maxSteps::Integer, thresh::Real = 1)\n\nOuter loop of the shibbs algorithm. Whitens data and loops untile diagonalization result is within threshold. Returns the dataset combined with transformation matrix as well as the transformation Matrix\n\nSee also whiten_dataset, estimate_cumulants, joint_diagonalize\n\n\n\n\n\n","category":"function"},{"location":"#ICA_BlindSourceSeparation.joint_diagonalize-Tuple{AbstractMatrix, Real, Integer}","page":"Home","title":"ICA_BlindSourceSeparation.joint_diagonalize","text":"joint_diagonalize(CM_in::AbstractMatrix, thresh::Real, max_iters::Integer)\n\nReturns diagonalization matrix and rotation size\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.l_bfgs_direction-NTuple{5, Any}","page":"Home","title":"ICA_BlindSourceSeparation.l_bfgs_direction","text":"l_bfgs_direction(G, h, s_list, y_list, r_list)\n\nCompute a search direction using the limited-memory BFGS (L-BFGS) algorithm.\n\nArguments\n\n`G::AbstractMatrix{<:Real}: the current gradient.\nh::AbstractMatrix{<:Real}: a diagonal approximation to the Hessian.\ns_list::Vector{AbstractMatrix{<:Real}}: list of previous update vectors.\ny_list::Vector{AbstractMatrix{<:Real}}: list of previous gradient differences.\nr_list::Vector{Float64}: list of scalars for each pair (s, y).\n\nReturns\n\nthe descent direction computed using the L-BFGS two-loop recursion.\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.line_search-NTuple{4, Any}","page":"Home","title":"ICA_BlindSourceSeparation.line_search","text":"function line_search(Y, direction, signs, current_loss; ls_tries)\n\nPerform a backtracking line search using a matrix exponential update.\n\nArguments\n\nY::AbstractMatrix{<:Real}: current signal matrix (N×T).\ndirection::AbstractMatrix{<:Real}: descent direction matrix of the same size as Y.\nsigns::AbstractVector{<:Real}: sign weights for the loss, same size as Y.\ncurrent_loss::Real: current loss value, or Inf to force recomputation.\nls_tries::Integer (keyword): maximum number of backtracking steps.\n\nReturns\n\nA tuple (converged, Y_new, new_loss, alpha) where\nconverged::Bool indicates whether a successful step was found,\nY_new::AbstractMatrix{<:Real} is the updated signal matrix (or original if no step succeeded),\nnew_loss::Real is the loss at Y_new,\nalpha::Real is the final step size.\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.loss-Tuple{Any, Any}","page":"Home","title":"ICA_BlindSourceSeparation.loss","text":"loss(Y, signs)\n\nCompute the total loss for a set of signals.\n\nArguments\n\nY::AbstractMatrix{<:Real}: matrix of shape N×T, where each row is a signal component over T samples.\nsigns::AbstractMatrix{<:Real}: matrix of the same shape as Y, containing signs or weights for each signal value.\n\nReturns\n\nA scalar representing the average contrast-based loss across all components and time steps.\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.plot_dataset-Tuple{sensorData}","page":"Home","title":"ICA_BlindSourceSeparation.plot_dataset","text":"plot_dataset(dataset::sensorData; title=\"Estimated Source Signals\")\n\nPlots each column of the dataset against the timestamp vector\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.proj_hessian_approx-Tuple{AbstractMatrix{<:Real}, AbstractVector{<:Real}, AbstractMatrix{<:Real}}","page":"Home","title":"ICA_BlindSourceSeparation.proj_hessian_approx","text":"proj_hessian_approx(Y, psidY_mean, G)\n\nCompute an approximation of the projected Hessian matrix.\n\nArguments\n\nY::AbstractMatrix{<:Real}: an N×T matrix of current signal components.\npsidY_mean::AbstractVector{<:Real}: a length-N vector containing the average of the second derivative (or negative squared derivative) of the contrast function for each component.\nG::AbstractMatrix{<:Real}: the gradient matrix of size N×N.\n\nReturns\n\nAn N×N symmetric matrix approximating the projected Hessian.\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.read_dataset-Tuple{String}","page":"Home","title":"ICA_BlindSourceSeparation.read_dataset","text":"read_dataset(filename::String) -> sensorData\n\nReads a file containing numbers separated by spaces or tabs. Number of columns is detected by analyzing the first valid line. Returns an instance of sensorData.\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.regularize_hessian-Tuple{AbstractMatrix{<:Real}, Real}","page":"Home","title":"ICA_BlindSourceSeparation.regularize_hessian","text":"regularize_hessian(h, lambda_min)\n\nClip the diagonal values of the Hessian approximation from below, ensuring all values are at least lambda_min.\n\nArguments\n\nh::AbstractMatrix{<:Real}: a diagonal matrix, where diagonal elements approximate eigenvalues.\nlambda_min::Real: minimum allowed eigenvalue\n\nReturns\n\nA matrix of the same size as h, with all values less than lambda_min replaced by lambda_min\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.score-Tuple{AbstractArray{<:Real}}","page":"Home","title":"ICA_BlindSourceSeparation.score","text":"score(Y::AbstractArray{<:Real})\n\nApply the hyperbolic tangent elementwise to each entry of Y.\n\nArguments\n\nY::AbstractArray{<:Real}: input array (vector, matrix, or higher‑dimensional array) of real numbers.\n\nReturns\n\nan array with the same shape as Y, where each element is tanh(y).\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.score_der-Tuple{AbstractMatrix{<:Real}}","page":"Home","title":"ICA_BlindSourceSeparation.score_der","text":"score_der(psiY::AbstractMatrix{<:Real})\n\nComputes the average derivative of the hyperbolic tangent nonlinearity for each row of psiY.\n\nArguments\n\npsiY::AbstractMatrix{<:Real}: input array of size N×T, where each row is a signal component over T observations.\n\nReturns\n\na vector in which each entry  represents the average derivative of the tanh nonlinearity, evaluated at each value in the corresponding row of psiY.\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.solve_hessian-Tuple{AbstractMatrix{<:Real}, AbstractMatrix{<:Real}}","page":"Home","title":"ICA_BlindSourceSeparation.solve_hessian","text":"solve_hessian(G, h)\n\nCompute the product of the inverse Hessian approximation with the gradient.\n\nArguments\n\nG::AbstractMatrix{<:Real}: the gradient matrix.\nh::AbstractMatrix{<:Real}: diagonal approximation of the Hessian.\n\nReturns\n\nA matrix of same size as G, where each element is G[i,j] / h[i,j].\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.sort_by_energy-Tuple{AbstractMatrix}","page":"Home","title":"ICA_BlindSourceSeparation.sort_by_energy","text":"sort_by_energy(B::AbstractMatrix)\n\nSort rows of B to put most energetic sources first Returns sorted matrix\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.whiten_dataset-Tuple{sensorData, Int64}","page":"Home","title":"ICA_BlindSourceSeparation.whiten_dataset","text":"whiten_dataset(X::sensorData, m::Int)\n\nApplies PCA whitening to a dataset, reducing its dimensionality to m components. Returns the whitened dataset (Txm), whitening matrix W (mxn), pseudo-inverse whitening matrix iW (nxm)\n\n\n\n\n\n","category":"method"},{"location":"#ICA_BlindSourceSeparation.whiten_dataset-Tuple{sensorData}","page":"Home","title":"ICA_BlindSourceSeparation.whiten_dataset","text":"whiten_dataset(X::sensorData)\n\nApplies PCA whitening to TxN data matrix. T: number of samples N: number of sensors Returns the whitened dataset.\n\n\n\n\n\n","category":"method"}]
}
